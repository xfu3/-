{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mp\n",
    "from matplotlib import cm\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from xgboost import XGBClassifier,XGBRegressor,plot_importance\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,cross_validate,cross_val_score,train_test_split\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions/Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(x,n):\n",
    "    val = 0\n",
    "    if x > n:\n",
    "        val = 1\n",
    "    return val\n",
    "\n",
    "class stats_met:\n",
    "    \"\"\"\n",
    "    定义一个类，用来分类器的性能度量\n",
    "    \"\"\"\n",
    "    def __init__(self, labels, scores):\n",
    "        \"\"\"\n",
    "        :param labels:数组类型，真实的标签\n",
    "        :param scores:数组类型，分类器的得分\n",
    "        \"\"\"\n",
    "        self.labels = labels\n",
    "        self.scores = scores\n",
    "        self.TP, self.FP, self.FN, self.TN = self.get_confusion_matrix()\n",
    "    \n",
    "    def accuracy(self):\n",
    "        \"\"\"\n",
    "        :return: 准确率\n",
    "        \"\"\"\n",
    "        accuracy = (self.TP + self.TN) / (self.TP + self.FN + self.FP + self.TN)\n",
    "        \n",
    "        return accuracy\n",
    " \n",
    "    def precision(self):\n",
    "        \"\"\"\n",
    "        :return: 精准度\n",
    "        \"\"\"\n",
    "        try:\n",
    "            precision = self.TP / (self.TP + self.FP)\n",
    "        except ZeroDivisionError:\n",
    "            precision = np.nan\n",
    "        \n",
    "        return precision\n",
    " \n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        :return: 召回率\n",
    "        \"\"\"\n",
    "        try:\n",
    "            recall = self.TP / (self.TP + self.FN)\n",
    "        except ZeroDivisionError:\n",
    "            recall = np.nan\n",
    "        \n",
    "        return recall\n",
    "    \n",
    "    def f_beta(self,beta):\n",
    "        precision = self.precision()\n",
    "        recall = self.recall()\n",
    "        \n",
    "        f_score = (1+beta**2)*((precision*recall/((beta**2)*precision+recall)))\n",
    "        return f_score\n",
    " \n",
    "    def get_confusion_matrix(self):\n",
    "        \"\"\"\n",
    "        计算混淆矩阵\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        tp, fp, fn, tn = 0., 0., 0., 0.\n",
    "        for i in range(len(self.labels)):\n",
    "            if self.labels[i] == 1 and self.scores[i] == 1:\n",
    "                tp += 1\n",
    "            elif self.labels[i] == 0 and self.scores[i] == 1:\n",
    "                fp += 1\n",
    "            elif self.labels[i] == 1 and self.scores[i] == 0:\n",
    "                fn += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        return [tp, fp, fn, tn]\n",
    "    \n",
    "    def get_eval_metrics(self):\n",
    "        print(\"Precision = {:0.3f},Recall = {:0.3f}, F_score = {:0.3f}\".format(self.precision(),self.recall(),self.f_beta(beta = 0.3)))\n",
    "    \n",
    "        \n",
    "    \n",
    "## Adjust graph spines\n",
    "def adjust_spines(ax, spines):\n",
    "    for loc, spine in ax.spines.items():\n",
    "        if loc in spines:\n",
    "            spine.set_position(('outward', 20))  # outward by 10 points\n",
    "            #spine.set_smart_bounds(True)\n",
    "        else:\n",
    "            spine.set_color('none')  # don't draw spine\n",
    "            \n",
    "## function of getting the optimized paramters and score\n",
    "def hypertuning_rscv(est,p_distr,nbr_iter,X,y):\n",
    "    rdmsearch = RandomizedSearchCV(est, param_distributions=p_distr,n_jobs=-1,scoring = 'roc_auc', n_iter=nbr_iter,cv=5)\n",
    "    \n",
    "    rdmsearch.fit(X,y)\n",
    "    ht_params = rdmsearch.best_params_\n",
    "    ht_score = rdmsearch.best_score_\n",
    "    return(ht_params,ht_score)\n",
    "\n",
    "def sort_feature_importance(importance,names):\n",
    "    #Create arrays from feature importance and feature names\n",
    "    feature_importance = np.array(importance)\n",
    "    feature_names = np.array(names)\n",
    "\n",
    "    #Create a DataFrame using a Dictionary\n",
    "    fea_data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "    fi_df = pd.DataFrame(fea_data)\n",
    "\n",
    "    #Sort the DataFrame in order decreasing feature importance\n",
    "    fi_df.sort_values(by=['feature_importance'],ascending = True,ignore_index=True,inplace = True)\n",
    "    return fi_df\n",
    "\n",
    "\n",
    "def visualization(recall,precision,f_score,threshold,threshold_max):\n",
    "    fig, ax = plt.subplots(1,1,figsize=(10,9))\n",
    "\n",
    "    colors = [plt.cm.tab10(i/float(5)) for i in range(3)]\n",
    "    \n",
    "    min_y = round(min(f_score),2)\n",
    "    max_y = round(max(f_score),2)\n",
    "    \n",
    "    min_y_per = round(min(f_score*100))\n",
    "    max_y_per = round(max(f_score*100))\n",
    "    \n",
    "    ax.plot(threshold,f_score,alpha = 0.3,color = colors[1],linewidth = \"1\")\n",
    "    ax.scatter(threshold,f_score,alpha = 0.9,color = colors[1],s = 0.38)\n",
    "    \n",
    "    ax.axvline(thre_max, color=\"black\", linestyle=\"dashed\")\n",
    "\n",
    "    ax.tick_params(axis= \"both\",direction = 'out',which='major', length=6.8,width=1, color='k',labelsize = 18)\n",
    "    ax.set_xticks([0.05*i for i in range(9,13)])\n",
    "    ax.set_yticks([0.01*i for i in range(min_y_per,max_y_per+1)])\n",
    "    ax.set_ylabel(\"F score\",size = 25,labelpad=12)\n",
    "    ax.set_xlabel(\"Threshold\",size = 25,labelpad=12)\n",
    "    ax.set_title(\"F_0.3 vs threshold\",size = 30)\n",
    "\n",
    "    ax.spines['left'].set_bounds(0.52,0.56)\n",
    "    ax.spines['bottom'].set_bounds(0.45,0.6)\n",
    "    ax.legend(fontsize = 25)\n",
    "\n",
    "    adjust_spines(ax, ['left', 'bottom'])\n",
    "    \n",
    "### K fold target encoding for training test\n",
    "class KFoldTargetEncoderTrain(BaseEstimator,TransformerMixin):\n",
    "\n",
    "    def __init__(self, colnames,targetName,n_fold=5,verbosity=True,discardOriginal_col=False):\n",
    "\n",
    "        self.colnames = colnames\n",
    "        self.targetName = targetName\n",
    "        self.n_fold = n_fold\n",
    "        self.verbosity = verbosity\n",
    "        self.discardOriginal_col = discardOriginal_col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self,X):\n",
    "\n",
    "        assert(type(self.targetName) == str)\n",
    "        assert(type(self.colnames) == str)\n",
    "        assert(self.colnames in X.columns)\n",
    "        assert(self.targetName in X.columns)\n",
    "\n",
    "        mean_of_target = X[self.targetName].mean()\n",
    "        kf = KFold(n_splits = self.n_fold, shuffle = True, random_state=2019)\n",
    "\n",
    "\n",
    "\n",
    "        col_mean_name = self.colnames + '_' + 'Kfold_Target_Enc'\n",
    "        X[col_mean_name] = np.nan\n",
    "\n",
    "        for tr_ind, val_ind in kf.split(X):\n",
    "            X_tr, X_val = X.iloc[tr_ind], X.iloc[val_ind]\n",
    "\n",
    "            X.loc[X.index[val_ind], col_mean_name] = X_val[self.colnames].map(X_tr.groupby(self.colnames)[self.targetName].mean())\n",
    "\n",
    "        X[col_mean_name].fillna(mean_of_target, inplace = True)\n",
    "\n",
    "        if self.verbosity:\n",
    "\n",
    "            encoded_feature = X[col_mean_name].values\n",
    "            print('Correlation between the new feature, {} and, {} is {}.'.format(col_mean_name,\n",
    "                                                                                      self.targetName,\n",
    "                                                                                      np.corrcoef(X[self.targetName].values, encoded_feature)[0][1]))\n",
    "        if self.discardOriginal_col:\n",
    "            X = X.drop(self.targetName, axis=1)\n",
    "            \n",
    "\n",
    "        return(X)\n",
    "    \n",
    "### K Fold target encoding for test set\n",
    "class KFoldTargetEncoderTest(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,train,colNames,encodedName):\n",
    "        \n",
    "        self.train = train\n",
    "        self.colNames = colNames\n",
    "        self.encodedName = encodedName\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        mean =  self.train[[self.colNames,\n",
    "                self.encodedName]].groupby(\n",
    "                                self.colNames).mean().reset_index() \n",
    "        \n",
    "        dd = {}\n",
    "        for index, row in mean.iterrows():\n",
    "            dd[row[self.colNames]] = row[self.encodedName]\n",
    "        X[self.encodedName] = X[self.colNames]\n",
    "        X = X.replace({self.encodedName: dd})\n",
    "    \n",
    "        return(X)\n",
    "    \n",
    "def model_train(pipe,param_dist,nbr_iter,X_train,y_train,target_train,X_cv,y_cv,target_cv,X_val,y_val,target_val,X_test,y_test,target_test,plot = False,plot_fea = False):\n",
    "    ## -------------------model training-------------------\n",
    "    start_time = time.time()\n",
    "    \n",
    "    random_params, random_score = hypertuning_rscv(pipe,param_dist,nbr_iter, X_cv, y_cv)\n",
    "    \n",
    "    classifier_1 = pipe.set_params(**random_params)\n",
    "    classifier_1.fit(X_cv,y_cv)\n",
    "    \n",
    "    \n",
    "    print(\"The selected set of hyperparameters[xgb] is {}.\".format(random_params))\n",
    "    ##-------------------F score extraction-------------------\n",
    "    \n",
    "    #pred_val = np.array(classifier_1.predict(X_val))\n",
    "    prob_val = np.array(classifier_1.predict_proba(X_val)[:,1])\n",
    "\n",
    "    interval = 0.2/10000\n",
    "    threshold = [0.4 + i*interval for i in range(10001)]\n",
    "\n",
    "    frame = pd.DataFrame()\n",
    "    scores_cla = [1]*X_val.shape[0]\n",
    "    \n",
    "    for i in threshold:    \n",
    "        scores_cla = [1 if j > i else 0 for j in prob_val]\n",
    "        \n",
    "        scores_val = list(scores_cla)\n",
    "        label_val = list(target_val)\n",
    "        \n",
    "        cl = stats_met(label_val,scores_val)\n",
    "        \n",
    "        precision = cl.precision()\n",
    "        \n",
    "        recall = cl.recall()\n",
    "        \n",
    "        f_beta = cl.f_beta(beta = 0.3)\n",
    "\n",
    "        frame_row = {\"threshold\":i,\"Precision\":precision,\"Recall\":recall,\"F_beta\":f_beta}\n",
    "        frame = frame.append(frame_row,ignore_index=True)\n",
    "    \n",
    "    \n",
    "    threshold_max = frame.loc[frame.F_beta == max(frame.F_beta),\"threshold\"].iloc[0]\n",
    "    \n",
    "    scores_val = [1 if j > threshold_max else 0 for j in prob_val]\n",
    "\n",
    "    labels_val = list(target_val)\n",
    "    \n",
    "    val_metrics = stats_met(labels_val,scores_val)\n",
    "    \n",
    "    print(\"---The metric scores of the validation set is---\")\n",
    "    val_metrics.get_eval_metrics()\n",
    "    \n",
    "    if plot:\n",
    "        threshold = frame[\"threshold\"]\n",
    "        f_score = frame[\"F_beta\"]\n",
    "        precision = frame[\"Precision\"]\n",
    "        recall = frame[\"Recall\"]\n",
    "\n",
    "        visualization(recall,precision,f_score,threshold,threshold_max)\n",
    "    \n",
    "    if plot_fea:\n",
    "        plt.rcParams['figure.figsize'] = [18,18]\n",
    "        plot_importance(classifier_1[2])\n",
    "        plt.show()  \n",
    "    \n",
    "    ## -------------------Training the whole dataset-------------------\n",
    "    classifier_2 = pipe.set_params(**random_params)\n",
    "    classifier_2.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "    pred_test = classifier_2.predict_proba(X_test)[:,1]\n",
    "\n",
    "    classifier_2.fit(X_train,y_train)\n",
    "\n",
    "    sma_barr_scores_1 = list(pred_test) \n",
    "    sma_barr_scores_2 = [1 if k > threshold_max else 0 for k in sma_barr_scores_1]\n",
    "\n",
    "    labels_test = list(target_test) ## delete expectancy\n",
    "    scores_test = sma_barr_scores_2\n",
    "\n",
    "    test_metrics = stats_met(labels_test,scores_test)\n",
    "\n",
    "    print(\"---The metric scores of the test set is---\")\n",
    "    test_metrics.get_eval_metrics()\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "july_poked = pd.read_csv('july_poked.csv',encoding = \"GBK\")\n",
    "aug_small_bucket = pd.read_csv('aug_small_bucket.csv',encoding = \"GBK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "july_poked.fillna(0,inplace = True)\n",
    "aug_small_bucket.fillna(0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "july_poked[\"key_2\"] = july_poked[\"city_code\"].astype(\"str\") + \"_\" + july_poked[\"l2_code\"].astype(\"str\") \n",
    "aug_small_bucket[\"key_2\"] = aug_small_bucket[\"city_code\"].astype(\"str\") + \"_\" + aug_small_bucket[\"l2_code\"].astype(\"str\") \n",
    "\n",
    "july_poked[\"key_3\"] = july_poked[\"city_code\"].astype(\"str\") + \"_\" + july_poked[\"l2_code\"].astype(\"str\") + \"_\" + july_poked[\"com_id\"].astype(\"str\")\n",
    "aug_small_bucket[\"key_3\"] = aug_small_bucket[\"city_code\"].astype(\"str\") + \"_\" + aug_small_bucket[\"l2_code\"].astype(\"str\") + \"_\" + aug_small_bucket[\"com_id\"].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "july_poked.drop([\"city_code\",'l2_code','com_id'],axis = 1,inplace = True)\n",
    "aug_small_bucket.drop([\"city_code\",'l2_code','com_id'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_dat = july_poked\n",
    "tot_dat_test = aug_small_bucket\n",
    "tot_dat_test_poked = tot_dat_test.loc[tot_dat_test.is_poked == \"poked\",:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_dat_small = july_poked.loc[july_poked[\"key_2\"].isin(list(aug_small_bucket[\"key_2\"].unique())),:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_dat_small.drop([\"is_poked\",\"sales\",\"del_exp_cnt\"],axis =1, inplace = True)\n",
    "tot_dat_test_poked.drop([\"is_poked\",\"sales\",\"del_exp_cnt\"],axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training\n",
    "tot_dat_small.loc[tot_dat_small[\"is_paid\"] == \"paid\",\"is_paid\"] = 1\n",
    "tot_dat_small.loc[tot_dat_small[\"is_paid\"] == \"unpaid\",\"is_paid\"] = 0\n",
    "\n",
    "tot_dat_small.loc[tot_dat_small[\"is_del\"] == \"deleted\",\"is_del\"] = 1\n",
    "tot_dat_small.loc[tot_dat_small[\"is_del\"] == \"undeleted\",\"is_del\"] = 0\n",
    "\n",
    "tot_dat_small[[\"is_paid\",\"is_del\"]] = tot_dat_small[[\"is_paid\",\"is_del\"]].apply(lambda x: x.astype('int'))\n",
    "\n",
    "## testing\n",
    "tot_dat_test_poked.loc[tot_dat_test_poked[\"is_paid\"] == \"paid\",\"is_paid\"] = 1\n",
    "tot_dat_test_poked.loc[tot_dat_test_poked[\"is_paid\"] == \"unpaid\",\"is_paid\"] = 0\n",
    "\n",
    "tot_dat_test_poked.loc[tot_dat_test_poked[\"is_del\"] == \"deleted\",\"is_del\"] = 1\n",
    "tot_dat_test_poked.loc[tot_dat_test_poked[\"is_del\"] == \"undeleted\",\"is_del\"] = 0\n",
    "\n",
    "tot_dat_test_poked[[\"is_paid\",\"is_del\"]] = tot_dat_test_poked[[\"is_paid\",\"is_del\"]].apply(lambda x: x.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sub_training and validation set preparation\n",
    "key2_train = tot_dat_small[\"key_2\"]\n",
    "key3_train = tot_dat_small[\"key_3\"]\n",
    "X_train = tot_dat_small.drop([\"is_paid\",\"key_2\",\"key_3\"],axis = 1)\n",
    "target_train = tot_dat_small[\"is_paid\"]\n",
    "y_train = tot_dat_small[\"is_del\"]\n",
    "\n",
    "X_cv, X_val, target_cv, target_val = train_test_split(X_train,target_train, test_size=0.3, random_state=18) \n",
    "\n",
    "y_cv = X_cv[\"is_del\"]\n",
    "y_val = X_val[\"is_del\"]\n",
    "\n",
    "X_train.drop(\"is_del\",axis = 1,inplace = True)\n",
    "X_cv.drop(\"is_del\",axis = 1,inplace = True)\n",
    "X_val.drop(\"is_del\",axis = 1,inplace = True)\n",
    "\n",
    "## test data preparation\n",
    "key2_test = tot_dat_test_poked[\"key_2\"]\n",
    "key3_test = tot_dat_test_poked[\"key_3\"]\n",
    "X_test = tot_dat_test_poked.drop([\"is_paid\",\"key_2\",\"key_3\"],axis =1)\n",
    "target_test = tot_dat_test_poked[\"is_paid\"]\n",
    "y_test = tot_dat_test_poked[\"is_del\"]\n",
    "\n",
    "X_test.drop(\"is_del\",axis = 1,inplace = True)\n",
    "# ## training\n",
    "# tot_dat_small\n",
    "\n",
    "# ## testing\n",
    "# tot_dat_test_poked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_random= {'svc__kernel': ['linear', 'rbf','poly'],\n",
    "            'svc__C': [0.1, 1, 10, 100, 1000],\n",
    "            'svc__gamma': [0.1, 1, 10, 100],\n",
    "            'svc__degree':[0, 1, 2, 3, 4, 5, 6], \n",
    "            'svc__random_state':[68]\n",
    "            }\n",
    "\n",
    "n_iter = 18\n",
    "\n",
    "pipe_svc = make_pipeline(StandardScaler(), SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train(pipe_svc,param_random,n_iter,X_train,y_train,target_train,X_cv,y_cv,target_cv,X_val,y_val,target_val,X_test,y_test,target_test,plot_fea = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
